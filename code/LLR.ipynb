{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b4072632-8177-4854-98f8-94ba9f96e758",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-01-12 11:46:45.295016: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 AVX512F AVX512_VNNI FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2023-01-12 11:46:45.447901: I tensorflow/core/util/util.cc:169] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
      "2023-01-12 11:46:45.488300: E tensorflow/stream_executor/cuda/cuda_blas.cc:2981] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "2023-01-12 11:46:46.245975: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer.so.7'; dlerror: libnvinfer.so.7: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /usr/local/nvidia/lib:/usr/local/nvidia/lib64\n",
      "2023-01-12 11:46:46.246049: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer_plugin.so.7'; dlerror: libnvinfer_plugin.so.7: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /usr/local/nvidia/lib:/usr/local/nvidia/lib64\n",
      "2023-01-12 11:46:46.246058: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Cannot dlopen some TensorRT libraries. If you would like to use Nvidia GPU with TensorRT, please make sure the missing libraries mentioned above are installed properly.\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import re\n",
    "\n",
    "import spacy\n",
    "from textstat.textstat import textstatistics\n",
    "spacy.load(\"en_core_web_sm\")\n",
    "\n",
    "\n",
    "# Returns the average number of syllables per\n",
    "# word in the text\n",
    "def avg_syllables_per_word(text):\n",
    "    syllable = syllables_count(text)\n",
    "    words = word_count(text)\n",
    "    ASPW = float(syllable) / float(words)\n",
    "    return ASPW\n",
    "\n",
    "# Textstat is a python package, to calculate statistics from\n",
    "# text to determine readability,\n",
    "# complexity and grade level of a particular corpus.\n",
    "# Package can be found at https://pypi.python.org/pypi/textstat\n",
    "def syllables_count(word):\n",
    "    return textstatistics().syllable_count(word)\n",
    "\n",
    "\n",
    "# Splits the text into sentences, using\n",
    "# Spacy's sentence segmentation which can\n",
    "# be found at https://spacy.io/usage/spacy-101\n",
    "def break_sentences(text):\n",
    "    nlp = spacy.load('en_core_web_sm')\n",
    "    doc = nlp(text)\n",
    "    return list(doc.sents)\n",
    " \n",
    "# Returns average sentence length\n",
    "def avg_sentence_length(text):\n",
    "    words = word_count(text)\n",
    "    sentences = sentence_count(text)\n",
    "    average_sentence_length = float(words / sentences)\n",
    "    return average_sentence_length\n",
    " \n",
    "# Returns Number of Words in the text\n",
    "def word_count(text):\n",
    "    sentences = break_sentences(text)\n",
    "    words = 0\n",
    "    for sentence in sentences:\n",
    "        words += len([token for token in sentence])\n",
    "    return words    \n",
    "    \n",
    "# Returns the number of sentences in the text\n",
    "def sentence_count(text):\n",
    "    sentences = break_sentences(text)\n",
    "    return len(sentences)\n",
    "\n",
    "\n",
    "# Return total Difficult Words in a text\n",
    "def difficult_words(text):\n",
    "     \n",
    "    nlp = spacy.load('en_core_web_sm')\n",
    "    doc = nlp(text)\n",
    "    # Find all words in the text\n",
    "    words = []\n",
    "    sentences = break_sentences(text)\n",
    "    for sentence in sentences:\n",
    "        words += [str(token) for token in sentence]\n",
    " \n",
    "    # difficult words are those with syllables >= 2\n",
    "    # easy_word_set is provide by Textstat as\n",
    "    # a list of common words\n",
    "    diff_words_set = set()\n",
    "     \n",
    "    for word in words:\n",
    "        syllable_count = syllables_count(str(word))\n",
    "        if word not in nlp.Defaults.stop_words and syllable_count >= 2:\n",
    "            diff_words_set.add(word)\n",
    " \n",
    "    return len(diff_words_set)\n",
    "\n",
    "# A word is polysyllablic if it has more than 3 syllables\n",
    "# this functions returns the number of all such words\n",
    "# present in the text\n",
    "def poly_syllable_count(text):\n",
    "    count = 0\n",
    "    words = []\n",
    "    text = str(text)\n",
    "    sentences = break_sentences(text)\n",
    "    for sentence in sentences:\n",
    "        words += [token for token in sentence]\n",
    "     \n",
    " \n",
    "    for word in words:\n",
    "        if len(word)>2:\n",
    "            syllable_count = syllables_count(str(word))\n",
    "            if syllable_count >= 3:\n",
    "                count += 1\n",
    "    return count\n",
    "\n",
    "\n",
    "df = pd.read_csv('word_rarity_list.csv')\n",
    "dict_size = df['word'].size\n",
    "\n",
    "def strip_character(a_string):\n",
    "    r = re.compile(r\"[^a-zA-Z- ]\")\n",
    "    return r.sub(' ', a_string)\n",
    "\n",
    "def remove_spaces(a_string):\n",
    "    return re.sub(' +', ' ', a_string)\n",
    "\n",
    "def remove_apos_s(a_string):\n",
    "    return re.sub(\"'s\", '', a_string)\n",
    "\n",
    "def clean_input_text(input_text):\n",
    "    # convert input to lowercase\n",
    "    input_text = input_text.lower()\n",
    "    # remove apostrophe-s from words\n",
    "    input_text = remove_apos_s(input_text)\n",
    "    # strip non-essential characters\n",
    "    input_text = strip_character(input_text)\n",
    "    # remove internal spaces\n",
    "    input_text = remove_spaces(input_text)\n",
    "    # remove end spaces\n",
    "    input_text = input_text.strip()\n",
    "    return input_text\n",
    "\n",
    "def tokenize(cleaned_text):\n",
    "    # split string on spaces\n",
    "    tokens = cleaned_text.split(\" \")\n",
    "    # create empty set\n",
    "    token_set = set()\n",
    "    # add one of each unqiue word from 'tokens' to set\n",
    "    for word in tokens:\n",
    "        if word in token_set:\n",
    "            pass\n",
    "        else:\n",
    "            token_set.add(word)\n",
    "    return token_set\n",
    "\n",
    "def flesch_reading_ease(text):\n",
    "    \n",
    "    text = str(text)\n",
    "    \"\"\"\n",
    "        Implements Flesch Formula:\n",
    "        Reading Ease score = 206.835 - (1.015 × ASL) - (84.6 × ASW)\n",
    "        Here,\n",
    "          ASL = average sentence length (number of words\n",
    "                divided by number of sentences)\n",
    "          ASW = average word length in syllables (number of syllables\n",
    "                divided by number of words)\n",
    "    \"\"\"\n",
    "    FRE = 206.835 - float(1.015 * avg_sentence_length(text)) -\\\n",
    "          float(84.6 * avg_syllables_per_word(text))\n",
    "    return FRE\n",
    "\n",
    "# returns rarity values of each word\n",
    "# frequency, zscore, count, index\n",
    "def fetch_rarity(input_text, type):\n",
    "    results = []\n",
    "    for word in input_text:\n",
    "        fetched_values = df[df['word'] == word]\n",
    "        if fetched_values.size == 0:\n",
    "            results.append((word, 0))\n",
    "        else:\n",
    "            if type == 'frequency':\n",
    "                results.append((word, fetched_values['frequency'].values[0]))\n",
    "            elif type == 'zscore':\n",
    "                results.append((word, fetched_values['zscore'].values[0]))\n",
    "            elif type == 'count':\n",
    "                results.append((word, fetched_values['count'].values[0]))\n",
    "            else:\n",
    "                results.append((word, fetched_values.index.values[0]))\n",
    "    return results\n",
    "\n",
    "def fetch_mean(tuple_list):\n",
    "    running_total = 0.0\n",
    "    size = len(tuple_list)\n",
    "    for tuple in tuple_list:\n",
    "        running_total += tuple[1]\n",
    "    list_mean = running_total/size\n",
    "    return list_mean\n",
    "\n",
    "def rare_finder(tuple_list, top, bottom):\n",
    "    rare_words = []\n",
    "    bottom_index = round(dict_size * (bottom/100))\n",
    "    top_index = round(dict_size * (top/100))\n",
    "    for tuple in tuple_list:\n",
    "        if tuple[1] >= top_index and tuple[1] <= bottom_index:\n",
    "            rare_words.append(tuple[0])\n",
    "        elif tuple[1] == 0 and tuple[0] != \"the\":\n",
    "            rare_words.append(tuple[0])\n",
    "    return rare_words\n",
    "\n",
    "#   'w' word mode returns rarity values for each word\n",
    "#   'a' aggregate mode returns average rarity values\n",
    "#   's' set-aggregate mode returns average rarity of only unique input values\n",
    "#   'f' finder mode returns words within rare range\n",
    "def word_rarity(input_text, mode='w', type='frequency', top=13, bottom=95):\n",
    "    if mode == 'f':\n",
    "        type = 'index'\n",
    "    cleaned_input_text = clean_input_text(input_text)\n",
    "    if mode == 'a':\n",
    "        tokens = cleaned_input_text.split(\" \")\n",
    "    else:\n",
    "        tokens = tokenize(cleaned_input_text)\n",
    "    token_values = fetch_rarity(tokens, type)\n",
    "    if mode == 'a' or mode == 's':\n",
    "        mean_of_tokens = fetch_mean(token_values)\n",
    "        return mean_of_tokens\n",
    "    elif mode == 'f':\n",
    "        rare_words = rare_finder(token_values, top, bottom)\n",
    "        return rare_words\n",
    "    else: #'w'\n",
    "        return token_values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "bf72e02c-5d48-4c02-8cd5-57cc88a8750f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.3377789128563968"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "word_rarity(\"the dog house\", mode='a')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "c17413ef-ebe0-4b33-8a51-07f8bba6f44c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.17227984024747503"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "word_rarity(s4, mode='a')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "257faab0-5250-4e5e-bd50-9f03dbad8393",
   "metadata": {},
   "outputs": [],
   "source": [
    "# X --> validation, train, test\n",
    "with open('wiki.X.raw') as f:\n",
    "    lines = f.readlines()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "4d55d659-d138-4fa2-868a-5242e29a6985",
   "metadata": {},
   "outputs": [],
   "source": [
    "lines_2 = []\n",
    "line = ''\n",
    "\n",
    "for i in range(len(lines)):\n",
    "    if i > 0:\n",
    "        if lines[i].startswith(\" = \") or lines[i].startswith(\" =\") or lines[i].startswith(\" =\"):\n",
    "            line = '** ' + lines[i]\n",
    "        else:\n",
    "            line = lines[i]\n",
    "        \n",
    "    lines_2.append(line)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "12b06c8e-dd95-42b3-8c3d-820d28af8bf8",
   "metadata": {},
   "outputs": [],
   "source": [
    "smallerlist = [l.replace('\\n','') for l in ' '.join(lines_2).split('** ')]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cfae4f75-99a8-49bf-9eb1-8dff70ac57fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from readability import Readability\n",
    "\n",
    "\n",
    "l_sentence = []\n",
    "l_word_count = []\n",
    "l_sentence_count = []\n",
    "l_rarity = []\n",
    "l_flesch_reading_ease = []\n",
    "\n",
    "i = 0\n",
    "\n",
    "for s in smallerlist:\n",
    "    s = str(s)\n",
    "    if len(s) > 120:\n",
    "        \n",
    "        sentence_count_value = -1 \n",
    "        word_count_value = -1\n",
    "        rarity = -1\n",
    "        fk_g_l = -1\n",
    "        try:\n",
    "            sentence_count_value = sentence_count(s)\n",
    "            rarity = word_rarity(s, mode='a')\n",
    "            word_count_value = word_count(s)\n",
    "            r = Readability(s)\n",
    "            fk = r.flesch_kincaid()\n",
    "            fk_g_l = fk.grade_level \n",
    "        except:\n",
    "            i = i+1\n",
    "        \n",
    "        l_sentence.append(s)\n",
    "        l_word_count.append(word_count_value) \n",
    "        l_sentence_count.append(sentence_count_value)\n",
    "        l_rarity.append(round(rarity, 3))\n",
    "        l_flesch_reading_ease.append(fk_g_l)    \n",
    "            \n",
    "\n",
    "df = pd.DataFrame(l_sentence, columns=['sentence'])\n",
    "\n",
    "\n",
    "df['word_count'] = l_word_count\n",
    "df['sentence_count'] = l_sentence_count\n",
    "df['rarity'] = l_rarity\n",
    "df['flesch_reading_ease'] = l_flesch_reading_ease\n",
    "\n",
    "\n",
    "\n",
    "df.to_csv('data.csv', index=False)\n",
    "\n",
    "\n",
    "print(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "901f3865-35ed-45f8-abca-ccadf60a633b",
   "metadata": {},
   "outputs": [],
   "source": [
    "max_word_count = df['word_count'].max()\n",
    "max_sentence_count = df['sentence_count'].max()\n",
    "max_rarity = df['rarity'].max()\n",
    "max_flesch_reading_ease = df['flesch_reading_ease'].max()\n",
    "\n",
    "\n",
    "min_word_count = 1\n",
    "min_sentence_count = 1\n",
    "min_rarity = df['rarity'].min()\n",
    "min_flesch_reading_ease = 1\n",
    "\n",
    "sum_word_count = df['word_count'].sum()\n",
    "sum_sentence_count = df['sentence_count'].sum()\n",
    "sum_rarity = df['rarity'].sum()\n",
    "sum_flesch_reading_ease = df['flesch_reading_ease'].sum()\n",
    "\n",
    "df.loc[df['flesch_reading_ease'] == -1, 'flesch_reading_ease'] = 14\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28316ac3-ff9b-47e2-bda9-217be5b048d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def aggregate_list(df):\n",
    "    aggregate_list = []\n",
    "    for i in range(len(df)):\n",
    "        word_count_norm = (df.iloc[i].word_count - min_word_count) / (max_word_count - min_word_count)\n",
    "        #sentence_count_norm = (df.iloc[i].sentence_count - min_sentence_count) / (max_sentence_count - min_sentence_count)\n",
    "        rarity_norm = (df.iloc[i].rarity - min_rarity) / (max_rarity - min_rarity)\n",
    "        flesch_reading_ease_norm = (df.iloc[i].flesch_reading_ease - min_flesch_reading_ease) / (max_flesch_reading_ease - min_flesch_reading_ease)\n",
    "\n",
    "        total_norm = word_count_norm + rarity_norm + flesch_reading_ease_norm\n",
    "        \n",
    "        aggregate_list.append(total_norm)\n",
    "    return aggregate_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d0d3195-a8f8-4051-9e13-f541fa2b9c8d",
   "metadata": {},
   "outputs": [],
   "source": [
    "df['aggregate_values'] = aggregate_list(df)\n",
    "df2 = df.sort_values(by='aggregate_values', ascending=True)\n",
    "#df = df.sort_values(by='sentence_count', ascending=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d6186b3-3f6b-4c38-9a63-3877a3e75678",
   "metadata": {},
   "outputs": [],
   "source": [
    "text = []\n",
    "for s in df2['sentence']:\n",
    "    if len(s)>2:\n",
    "        s = str(s)\n",
    "        text.append(s.replace('\\n',''))\n",
    "\n",
    "# X --> validation, train, test\n",
    "textfile = open(\"wiki.X.raw\", \"w\")\n",
    "for element in text:\n",
    "    element  = str(element)\n",
    "    textfile.write(element)\n",
    "textfile.close()   "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
